{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Рекуррентные нейросети для классификации текстов.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH0ICwPAbtRF"
      },
      "source": [
        "#Рекуррентные нейросети для классификации текстов\n",
        "В этом уроке мы рассмотрим различные компоненты, которые помогут нам создать полноценный классификатор последовательностей (текстов) в будущем.\n",
        "\n",
        "В том числе рассмотрим реализацию двунаправленных LSTM сетей."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2xboRXueFMq"
      },
      "source": [
        "### Используем TensorFlow 2.0\n",
        "\n",
        "На момент подготовки этих материалов в Google Colab по умолчанию используется версия TensorFlow 1.X\n",
        "\n",
        "Переключаемся на версию 2.0 (работает только в Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "carDuaZIHioO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e2ee95bb-4f39-4281-81ae-b5409d9be649"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oRPBIvxeLuG"
      },
      "source": [
        "### Загрузка библиотек\n",
        "TensorFlow должен иметь как минимум версию 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-zMowjHFz3t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acf2bfa8-eee6-4e8b-dcf5-ae14ebeb0447"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rHkfQvohJQy"
      },
      "source": [
        "### Входной тензор\n",
        "Как и раньше, создадим тензор входных тестовых цепочек. \n",
        "\n",
        "Однако, теперь, давайте работать не с произвольными векторами, а со словами.\n",
        "Зачастую нам дан (или мы можем создать) словарь из всех используемых слов. Тогда каждое слово можно кодировать индексом в этом словаре (просто целым числом). Предобработку текста (нормализацию слов итд) оставим за рамками.\n",
        "\n",
        "В таком случае входной тензор будет просто двумерным целочисленным тензором (каждый его элемент - индекс слова в словаре).\n",
        "\n",
        "Размерности тензора `x`: (батч, длина цепочки)\n",
        "\n",
        "Но так как в рекуррентную нейронную сеть нам всё равно надо передавать векторные представления слов, нам далее потребуется дополнительный шаг -- получение эмбеддингов. Один из самых простых способов сделать это -- создать обучаемый `Embedding` слой, который будет просто содержать матрицу из эмбеддингов для каждого слова из словаря. Для создания такого слоя нам понядобятся параметры `EMB_SIZE` (размерность эмбеддинга) и `VOCAB_SIZE` (количество слов в словаре).\n",
        "\n",
        "Мы всё еще не решаем никакую конкретную задачу, а просто демонстрируем то, как можно создать и использовать различные рекуррентные сети на абстрактных тестовых данных.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REGfFKjnP_RD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e84d5ec7-ccb1-42b8-c612-cc7048fde76c"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "SEQ_LEN = 100 # Длина последовательности\n",
        "EMB_SIZE = 16 # Размер векторного представления (эмбеддинга)\n",
        "VOCAB_SIZE = 10000 # Количество слов в словаре\n",
        "\n",
        "x = np.random.randint(VOCAB_SIZE, size=(BATCH_SIZE, SEQ_LEN))\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpXmRTuSwttk"
      },
      "source": [
        "### Обучаемый Embedding слой\n",
        "Создадим Embedding слой, который отображает словарный индекс слова в соответствующий эмбеддинг (вектор).\n",
        "\n",
        "По сути, это просто обучаемая матрица размера `(VOCAB_SIZE, EMB_SIZE)`, которая по индексу слова `i` выдает i-ый столбец этой матрицы.\n",
        "\n",
        "Такой слой можно использовать как простую альтернативу Word2Vec и других векторных представлений слов.\n",
        "\n",
        "Если мы применим Embedding слой для нашего входного тензора, получим уже эмбеддинги (векторы) для каждого слова в цепочке. То есть выходной тензор будет иметь размер `(BATCH_SIZE, SEQ_LEN, EMB_SIZE)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EWjVu7xQdcB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d6841628-08b7-49d1-8baf-da3057dfbc36"
      },
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE)\n",
        "\n",
        "y = embedding_layer(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 100, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZwxp7dcxleP"
      },
      "source": [
        "### Пример простой рекуррентной нейронной сети для бинарной классификации текстов\n",
        "Теперь у нас есть почти всё, что нужно для создания классификатора текстов.\n",
        "\n",
        "Один из самых простых классификаторов можно создать из следующих трёх блоков: Embedding слой, рекуррентный слой, который возвращает лишь один вектор для каждой цепочки (`return_sequences=False`), Полносвязный слой, отображающий полученный промежуточный вектор в классификационный ответ. Например, если у нас бинарный классификация (на два класса), можно в конце использовать один нейрон с функцией активации sigmoid (как в обычных нейросетевых классификаторах)\n",
        "\n",
        "Создадим соответствующую Sequential модель, вызовем инференс и посмотрим на размер ответа. \n",
        "\n",
        "У нас на входе был батч из цепочек. Каждую цепочку мы хотим проклассифицировать. Ответ классификатора для цепочки это одно число (вероятность после сигмоида). Тогда размер выходного тензора должен получиться `(BATCH_SIZE, 1)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8qAIT44QuRX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a4cbe152-c255-4948-a280-ed48bb482c2c"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 16),\n",
        "    tf.keras.layers.LSTM(16, return_sequences=False),\n",
        "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "y = model(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1X9u-7_x50p"
      },
      "source": [
        "### Регуляризация\n",
        "Рассмотрим различные модификации и улучшения текстового классификатора.\n",
        "\n",
        "Рекуррентные сети часто страдают от проблемы переобучения, поэтому к ним необходимо применять один из стандартных способов борьбы с переобучением -- dropout (случайное отключение части нейронов во время обучения).\n",
        "\n",
        "В LSTM слое можно устанавливать различные показатели дропаута на разные участки слоя с помощью параметров `dropout` и `recurrent_dropout`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DHIPnTCxvsO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7efa409d-f600-484f-d6b4-77f1155b1585"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 16),\n",
        "    tf.keras.layers.LSTM(16, return_sequences=False, dropout=0.5, recurrent_dropout=0.5),\n",
        "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "y = model(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox4DluGkymch"
      },
      "source": [
        "### Более глубокая рекуррентная сеть\n",
        "Можно ставить несколько LSTM слоёв, делая сеть глубже.\n",
        "\n",
        "В таком случае, все кроме последнего LSTM слоя должны иметь параметр `return_sequences=True`, иначе последующий слой не сможет обработать выход предыдущего (он ожидает последовательность, а ему дают лишь один объект)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAqsi2w5Q6WB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "26896637-e6d7-4eba-cde0-9df7bc38e69c"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 16),\n",
        "    tf.keras.layers.LSTM(16, return_sequences=True, dropout=0.5, recurrent_dropout=0.5),\n",
        "    tf.keras.layers.LSTM(16, return_sequences=False, dropout=0.5, recurrent_dropout=0.5),\n",
        "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "y = model(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0jCY2guzI1F"
      },
      "source": [
        "### Двунаправленные рекуррентные сети\n",
        "До этого моменты мы использовали однонаправленные рекуррентные сети. В случае классификации текстов могут помочь улучшить результат двунаправленные сети (которые считывают входную цепочку в двух противоположных направлениях).\n",
        "\n",
        "В Keras двунаправленность можно добавить просто обернув LSTM слой в `tf.keras.layers.Bidirectional()`.\n",
        "\n",
        "Сделаем это для всех наших LSTM слоёв."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI6GQG-cPMC8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "300c3ebd-f430-4197-ddcf-80b9b5d03f37"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(VOCAB_SIZE, 16),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(16, return_sequences=True, dropout=0.5, recurrent_dropout=0.5)),\n",
        "    tf.keras.layers.Bidirectional(\n",
        "        tf.keras.layers.LSTM(16, return_sequences=False, dropout=0.5, recurrent_dropout=0.5)),\n",
        "    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid),\n",
        "])\n",
        "\n",
        "y = model(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiCUi45PLI2y"
      },
      "source": [
        "###Заключение\n",
        "Рассмотренных элементов достаточно для построения и обучения полноценного классификатора текстов, чем мы и займёмся в следующем уроке."
      ]
    }
  ]
}