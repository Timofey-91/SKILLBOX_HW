{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Реализация слоя RNN. Решение практики.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgshItiNbYmH"
      },
      "source": [
        "#Реализация слоя RNN\n",
        "\n",
        "В этом уроке мы посмотрим, как реализовать простой RNN слой. Причём, сделаем это двумя способами. Сначала будем использовать готовую реализацию из Keras, а затем реализуем свой RNN слой с нуля на чистом TensorFlow, чтобы лучше понять, как он устроен внутри. Затем сравним результаты работы двух версий RNN слоя, чтобы убедиться, что они эквивалентны."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ScVSbBleHiq"
      },
      "source": [
        "### Используем TensorFlow 2.0\n",
        "\n",
        "На момент подготовки этих материалов в Google Colab по умолчанию используется версия TensorFlow 1.X\n",
        "\n",
        "Переключаемся на версию 2.0 (работает только в Colab)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixxLAjOiA4S4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1b10ae45-9f90-46d1-b70c-05a113bb3919"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1c9g1MHeUCl"
      },
      "source": [
        "### Загрузка библиотек\n",
        "TensorFlow должен иметь как минимум версию 2.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-zMowjHFz3t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3f12c31-b32b-438e-db25-67312c13ae4d"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAGOjhbmg409"
      },
      "source": [
        "### Входной тензор\n",
        "Подготовим данные, на которых будем тестировать наш RNN слой. Пока что мы не решаем никакую конкретную задачу и ничего не обучаем. Мы просто хотим посмотреть, как работает прямое распространение RNN слоя (и сравнить две версии), поэтому нам подойдут просто случайные данные (случайный входной тензор).\n",
        "\n",
        "В работе с рекуррентными нейронными сетями на входе, как правило, используется трёхмерный тензор. Первое измерение - батч (это измерение есть почти всегда, когда мы работаем с любыми нейронными сетями). Его смысл всегда в том, что мы одновременно обрабатываем пакет (группу) данных вместо одного экземпляра. Второе измерение - измерение последовательности. То есть, если наша последовательность была длины 100, то второе измерение будет 100. И третье измерение - размер векторного представления (эмбеддинга) каждого элемента последовательности.\n",
        "\n",
        "Если в реальной задаче цепочки в пределах одного батча имеют разную длину (например, это предложения с разным количеством слов), то обычно все цепочки предварительно приводят к равным длинам за счёт добавления нулевых векторов (или других специальных векторов, трактуемых как элементы паддинга цепочки), чтобы мы смогли собрать нужный нам тензор.\n",
        "\n",
        "Размерности тензора `x`: (батч, длина цепочки, размер эмбеддинга)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIprlbAyJo18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5052cc8-7d11-4db3-b882-81779e2fae1f"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "SEQ_LEN = 100 # Длина последовательности\n",
        "EMB_SIZE = 16 # Размер векторного представления (эмбеддинга)\n",
        "\n",
        "x = np.random.rand(BATCH_SIZE, SEQ_LEN, EMB_SIZE).astype(np.float32)\n",
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 100, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a8luH2CinU9"
      },
      "source": [
        "### Создание простого RNN слоя с помощью Keras\n",
        "Для начала создадим и протестируем обычный RNN слой из Keras.\n",
        "\n",
        "Для его создания нам понадобятся следующие параметры:\n",
        "\n",
        "`H_SIZE` - количество элементов в выходном векторе `h`.\n",
        "\n",
        "`activation` - функция активации для слоя\n",
        "\n",
        "`return_sequences` - флаг, в зависимости от которого на выходе мы получаем цепочку векторов `h` (той же длины, что и входная цепочка) или только последний вектор `h`\n",
        "\n",
        "В этом примере мы будем использовать такой RNN слой, который возвращает выходной вектор `h` для каждого элемента выходной последовательности. То есть, на входе последовательность длины `SEQ_LEN`, на выходе последовательность `SEQ_LEN`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhN_5VeyHrZc"
      },
      "source": [
        "H_SIZE = 32\n",
        "rnn_layer = tf.keras.layers.SimpleRNN(H_SIZE, activation='relu', return_sequences=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9kamObVGkdM"
      },
      "source": [
        "Запускаем инференс (прямое распространение) для созданного слоя и проверяем размерность выходного тензора.\n",
        "\n",
        "Она должна была получиться `(BATCH_SIZE, SEQ_LEN, H_SIZE)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL_A90iGH2Pd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ed93b09-c5a1-4acb-dd9f-661eaffaaf8c"
      },
      "source": [
        "y = rnn_layer(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 100, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-wUEHfWkEpp"
      },
      "source": [
        "### Создание простого RNN слоя с помощью TensorFlow\n",
        "Теперь давайте создадим свой RNN слой на чистом TensorFlow, а затем сравним результат его работы с версией из Keras.\n",
        "\n",
        "Отдельно выделим функцию для так называемой RNN ячейки, задача которой сделать прямое распространение в данный момент времени `t`. То есть, по входу `x` и вектору `h` с предыдущего момента времени получить новый вектор `h`.\n",
        "\n",
        "И тогда останется просто пройтись в цикле по входной последовательности и вызвать RNN ячейку для каждого её элемента (передавая при этом текущий полученный вектор `h` в следующий запуск ячейки).\n",
        "\n",
        "Вспомним, какие обучаемые параметры нам нужны для простого RNN слоя: две матрицы и один вектор смещений (биас). Для этого в конструкторе создадим два полносвязных слоя и у второго выключим `bias` (нам будет достаточно биаса из первого слоя)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0CrXfk4LKe4"
      },
      "source": [
        "class RNNLayer(tf.keras.Model):\n",
        "    def __init__(self, h_size):\n",
        "        super().__init__()\n",
        "        self.h_size = h_size\n",
        "        self.fcXH = tf.keras.layers.Dense(self.h_size)\n",
        "        self.fcHH = tf.keras.layers.Dense(self.h_size, use_bias=False) # биас не нужен, так как он есть в fcXH\n",
        "\n",
        "    # RNN ячейка\n",
        "    def RNN_cell(self, x, h):\n",
        "        h = tf.nn.relu(self.fcXH(x) + self.fcHH(h))\n",
        "        return h\n",
        "\n",
        "    def call(self, x_all):\n",
        "        batch, length, emb_size = x.shape\n",
        "        h = tf.zeros((batch, self.h_size))\n",
        "        h_all = [] # список всех получившихся векторов h\n",
        "        \n",
        "        # Цикл по входной последовательности\n",
        "        for i in range(length):\n",
        "            h = self.RNN_cell(x_all[:, i, :], h)\n",
        "            h_all.append(h)\n",
        "            \n",
        "        # склеиваем все ответы и меняем размерности, чтоб получилось (batch, length, h_size)\n",
        "        h_all = tf.transpose(tf.stack(h_all), [1, 0, 2])\n",
        "        return h_all\n",
        "\n",
        "rnn_layer_my = RNNLayer(H_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtYzm68tHkYX"
      },
      "source": [
        "Запускаем инференс (прямое распространение) для созданного слоя и проверяем размерность выходного тензора.\n",
        "\n",
        "Она должна была получиться `(BATCH_SIZE, SEQ_LEN, H_SIZE)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQOAidLLOUB2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d96c2cb-6d49-46c5-9de4-087a04c21310"
      },
      "source": [
        "y = rnn_layer_my(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 100, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJluutj0ls6H"
      },
      "source": [
        "### Проверка эквивалентности двух реализаций\n",
        "Теперь проверим эквивалентность двух созданных RNN слоёв (из Keras и нашего собственного).\n",
        "\n",
        "Пока что сравнить их ответы мы не можем, так как у каждой версии слоя при его создании веса инициализировались случайным образом. Параметры инициализируются либо во время первого инференса, либо после вызова `model.build(...)`.\n",
        "\n",
        "Для того, чтобы наши слои работали одинаково, надо скопировать веса из одного в другой.\n",
        "\n",
        "Скопируем веса из `rnn_layer` в наш `rnn_layer_my`, после чего сделаем прямое распространение для обоих реализаций и сравним результаты. Разница должна получиться нулевой (или очень маленькой)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_HT9VKdM1HA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4051888-2a42-42a0-b7ed-117873b29641"
      },
      "source": [
        "# Перед тем, как что-то присваивать в параметры модели, нужно чтобы они создались.\n",
        "# Для этого можно вызвать либо инференс с каким-то входом, либо model.build(...)\n",
        "\n",
        "rnn_layer_my.fcXH.kernel = rnn_layer.weights[0]\n",
        "rnn_layer_my.fcHH.kernel = rnn_layer.weights[1]\n",
        "rnn_layer_my.fcXH.bias = rnn_layer.weights[2]\n",
        "\n",
        "y = rnn_layer(x)\n",
        "y_my = rnn_layer_my(x)\n",
        "\n",
        "print(np.max(np.abs(y.numpy() - y_my.numpy())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd1LSLgIOuj5"
      },
      "source": [
        "Очень часто RNN ячейка является отдельной сущностью. Особенно это удобно, если у неё довольно сложное строение, и если мы не хотим вдаваться в детали её реализации, а просто хотим строить рекуррентную сеть из некоторых ячеек. Чтобы закрепить эту информацию, выполните следующие задания:\n",
        "\n",
        "**[Задание 1]** Разделите реализации RNN ячейки и RNN слоя. Другими словами, реализуйте отдельный класс `RNNCell` (унаследованный от `tf.keras.Model`) и перенесите туда все обучаемые параметры RNN слоя. Задача ячейки (при инференсе) по векторам x и h предсказывать новый вектор h. А в новом классе `RNNLayer` вместо полносвязных слоёв сразу создайте экземпляр класса `RNNCell` и используйте его для обработки входной последовательности всё в том же цикле, что и раньше. \n",
        "\n",
        "**[Задание 2]** Сравните результаты работы нового `RNNLayer` с оригинальным RNN слоем из Keras также, как мы это делали раньше (разница предсказаний должна получиться равной нулю)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWdAZmnSO2hG"
      },
      "source": [
        "class RNNCell(tf.keras.Model):\n",
        "    def __init__(self, h_size):\n",
        "        super().__init__()\n",
        "        self.h_size = h_size\n",
        "        self.fcXH = tf.keras.layers.Dense(self.h_size)\n",
        "        self.fcHH = tf.keras.layers.Dense(self.h_size, use_bias=False)\n",
        "        \n",
        "    def call(self, x, h):\n",
        "        h = tf.nn.relu(self.fcXH(x) + self.fcHH(h))\n",
        "        return h\n",
        "    \n",
        "class RNNLayer(tf.keras.Model):\n",
        "    def __init__(self, h_size):\n",
        "        super().__init__()\n",
        "        self.h_size = h_size\n",
        "        self.rnn_cell = RNNCell(h_size)\n",
        "\n",
        "    def call(self, x_all):\n",
        "        batch, length, emb_size = x.shape\n",
        "        h = tf.zeros((batch, self.h_size))\n",
        "        h_all = []\n",
        "        \n",
        "        for i in range(length):\n",
        "            h = self.rnn_cell(x_all[:, i, :], h)\n",
        "            h_all.append(h)\n",
        "            \n",
        "        h_all = tf.transpose(tf.stack(h_all), [1, 0, 2])\n",
        "        return h_all\n",
        "\n",
        "rnn_layer_my = RNNLayer(H_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBk8-DrtPudh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6396c18f-4ef3-4eb9-f938-dcd517cd8825"
      },
      "source": [
        "y = rnn_layer_my(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2, 100, 32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epqYZ3AyPxGn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e0f7c10-8f1b-43a5-daa8-f2622d8a62b6"
      },
      "source": [
        "# Перед тем, как что-то присваивать в параметры модели, нужно чтобы они создались.\n",
        "# Для этого можно вызвать либо инференс с каким-то входом, либо model.build(...)\n",
        "\n",
        "rnn_layer_my.rnn_cell.fcXH.kernel = rnn_layer.weights[0]\n",
        "rnn_layer_my.rnn_cell.fcHH.kernel = rnn_layer.weights[1]\n",
        "rnn_layer_my.rnn_cell.fcXH.bias = rnn_layer.weights[2]\n",
        "\n",
        "y = rnn_layer(x)\n",
        "y_my = rnn_layer_my(x)\n",
        "\n",
        "print(np.max(np.abs(y.numpy() - y_my.numpy())))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}