{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "15_dialog_generation_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40WulyLcjkEW"
      },
      "source": [
        "# Создание чат-бота с использованием модели Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpw0dLZY5Mkn"
      },
      "source": [
        "# Загрузка библиотек\n",
        "from __future__ import print_function\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "from keras.layers import Input, Embedding, LSTM, TimeDistributed, Dense, Bidirectional\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot_tJ3-iUBxz"
      },
      "source": [
        "# Загрузка и предварительная обработка данных:\n",
        "\n",
        "* Следующим шагом является переформатирование нашего файла данных и загрузка данных в структуры, с которыми мы можем работать.\n",
        "* Набор данных \"Cornell Movie-Dialogs Corpus\" состоит из следующего описания:\n",
        " * 220 579 разговоров между 10 292 парами героев фильмов\n",
        " * 9035 персонажей из 617 фильмов\n",
        " * 304,713 всего высказываний\n",
        " * Этот набор данных большой и разнообразный, и существует большое разнообразие языковых формальностей, периодов времени, настроений и т. д.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAyvtzg0fysR",
        "outputId": "db53cfde-fed2-4618-a6f8-f1ddd9efe5ed"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSXsxtuE0Dpr"
      },
      "source": [
        "conv_lines = open('./drive/My Drive/ChatBot_Dataset/movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "lines = open('./drive/My Drive/ChatBot_Dataset/movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\\n')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXiwE91L_UH8"
      },
      "source": [
        "id2line = {}\n",
        "for line in lines:\n",
        "    _line = line.split(' +++$+++ ')\n",
        "    if len(_line) == 5:\n",
        "        id2line[_line[0]] = _line[4]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp0ZvkP8JlFf"
      },
      "source": [
        "convs = []\n",
        "for line in conv_lines[:-1]:\n",
        "    _line = line.split(' +++$+++ ')[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
        "    convs.append(_line.split(','))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVHiSXjZSJy3",
        "outputId": "de1dfd22-c881-4e78-de55-11c659585980"
      },
      "source": [
        "for k in convs[300]:\n",
        "    print (k, id2line[k])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L3490 That's what he did to me.  He put cigarettes out on me.\n",
            "L3491 Your father put cigarettes out on you?\n",
            "L3492 Out on my back when I was a small boy.\n",
            "L3493 Can I see your back?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuIrDCxLSyt4",
        "outputId": "0e8921a6-916e-4cfc-d716-786d8d3a3539"
      },
      "source": [
        "# Sort the sentences into questions (inputs) and answers (targets)\n",
        "questions = []\n",
        "answers = []\n",
        "for conv in convs:\n",
        "    for i in range(len(conv)-1):\n",
        "        questions.append(id2line[conv[i]])\n",
        "        answers.append(id2line[conv[i+1]])\n",
        "        \n",
        "# Compare lengths of questions and answers\n",
        "print(len(questions))\n",
        "print(len(answers))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "221616\n",
            "221616\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcjaGSU7LAoj"
      },
      "source": [
        "def clean_text(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|]\", \"\", text)\n",
        "#     text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    text = \" \".join(text.split())\n",
        "    return text"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb168S6Xekh0"
      },
      "source": [
        "# Очищаем данные\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "  clean_questions.append(clean_text(question))\n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "  clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-eArYa1_euj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38fb62c3-6954-4887-ab39-12ee78a42697"
      },
      "source": [
        "# Найдем длину предложений (не используя nltk из-за скорости обработки)\n",
        "lengths = []\n",
        "# lengths.append([len(nltk.word_tokenize(sent)) for sent in clean_questions])\n",
        "for question in clean_questions:\n",
        "    lengths.append(len(question.split()))\n",
        "for answer in clean_answers:\n",
        "    lengths.append(len(answer.split()))\n",
        "# Создадим dataframe, что бы можно было просмотреть значения данных.\n",
        "lengths = pd.DataFrame(lengths, columns=['counts'])\n",
        "print(np.percentile(lengths, 80))\n",
        "print(np.percentile(lengths, 85))\n",
        "print(np.percentile(lengths, 90))\n",
        "print(np.percentile(lengths, 95))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.0\n",
            "19.0\n",
            "24.0\n",
            "32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wwe-737OBqYq",
        "outputId": "0ea33255-24af-44c6-ebac-a757d5e26464"
      },
      "source": [
        "# Удалим вопросы и ответы, которые короче 1 слова и длиннее 20 слов.\n",
        "min_line_length = 2\n",
        "max_line_length = 20\n",
        "\n",
        "# Отфильтруем слишком длинные и короткие вопросы\n",
        "short_questions_temp = []\n",
        "short_answers_temp = []\n",
        "\n",
        "for i, question in enumerate(clean_questions):\n",
        "    if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:\n",
        "        short_questions_temp.append(question)\n",
        "        short_answers_temp.append(clean_answers[i])\n",
        "\n",
        "# Отфильтруем слишком длинные и короткие вопросы\n",
        "short_questions = []\n",
        "short_answers = []\n",
        "\n",
        "for i, answer in enumerate(short_answers_temp):\n",
        "    if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:\n",
        "        short_answers.append(answer)\n",
        "        short_questions.append(short_questions_temp[i])\n",
        "        \n",
        "print(len(short_questions))\n",
        "print(len(short_answers))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "138528\n",
            "138528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCL06AlzCAat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06e920b5-1842-40cf-f0c2-693961aecf3b"
      },
      "source": [
        "r = np.random.randint(1,len(short_questions))\n",
        "\n",
        "for i in range(r, r+3):\n",
        "    print(short_questions[i])\n",
        "    print(short_answers[i])\n",
        "    print()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "burn them.\n",
            "and the world the whole world.\n",
            "\n",
            "and the world the whole world.\n",
            "burn it all.\n",
            "\n",
            "got a cause?\n",
            "are the glory boys actually showing interest in investigationgs work? i may have a stroke.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Wl0Hj7JCa8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cec0136-129e-4ada-f110-ca5dcb721409"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "num_samples = 20000 \n",
        "short_questions = short_questions[:num_samples]\n",
        "short_answers = short_answers[:num_samples]\n",
        "#токенизация\n",
        "short_questions_tok = [nltk.word_tokenize(sent) for sent in short_questions]\n",
        "short_answers_tok = [nltk.word_tokenize(sent) for sent in short_answers]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBCd6s-HYTd3",
        "outputId": "b572e216-c7f1-48b8-ff41-eb7d57b26e41"
      },
      "source": [
        "#train-validation split\n",
        "data_size = len(short_questions_tok)\n",
        "\n",
        "training_input  = short_questions_tok[:round(data_size*(80/100))]\n",
        "training_input  = [tr_input[::-1] for tr_input in training_input] #обратная последовательность ввода для лучшей производительности\n",
        "training_output = short_answers_tok[:round(data_size*(80/100))]\n",
        "\n",
        "validation_input = short_questions_tok[round(data_size*(80/100)):]\n",
        "validation_input  = [val_input[::-1] for val_input in validation_input] #обратная последовательность ввода для лучшей производительности\n",
        "validation_output = short_answers_tok[round(data_size*(80/100)):]\n",
        "\n",
        "print('training size', len(training_input))\n",
        "print('validation size', len(validation_input))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training size 16000\n",
            "validation size 4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk4HC8RoDYE4"
      },
      "source": [
        "# encoder&decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXhD1DusC30r"
      },
      "source": [
        "vocab = {}\n",
        "for question in short_questions_tok:\n",
        "    for word in question:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1\n",
        "\n",
        "for answer in short_answers_tok:\n",
        "    for word in answer:\n",
        "        if word not in vocab:\n",
        "            vocab[word] = 1\n",
        "        else:\n",
        "            vocab[word] += 1           "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D3tP2SEDc_f"
      },
      "source": [
        "threshold = 15\n",
        "count = 0\n",
        "for k,v in vocab.items():\n",
        "    if v >= threshold:\n",
        "        count += 1"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yksu9QtSC38O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d77b074-fc75-412d-f249-b314c3eaff5c"
      },
      "source": [
        "print(\"Size of total vocab:\", len(vocab))\n",
        "print(\"Size of vocab we will use:\", count)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of total vocab: 13197\n",
            "Size of vocab we will use: 1447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFk_hu4blGIm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27bfeaca-9d84-40ca-fd16-6722d3bab7ef"
      },
      "source": [
        "WORD_CODE_START = 1\n",
        "WORD_CODE_PADDING = 0\n",
        "\n",
        "\n",
        "word_num  = 2 \n",
        "encoding = {}\n",
        "decoding = {1: 'START'}\n",
        "for word, count in vocab.items():\n",
        "    if count >= threshold:\n",
        "        encoding[word] = word_num \n",
        "        decoding[word_num ] = word\n",
        "        word_num += 1\n",
        "\n",
        "print(\"No. of vocab used:\", word_num)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of vocab used: 1449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYAqrQi-lTu2"
      },
      "source": [
        "decoding[len(encoding)+2] = '<UNK>'\n",
        "encoding['<UNK>'] = len(encoding)+2"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvdcvblAlcnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eedf518-9366-4857-8f64-56451c77ec68"
      },
      "source": [
        "dict_size = word_num+1\n",
        "dict_size"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1450"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wmq_UDBClfs_"
      },
      "source": [
        "# Vectorizing dataset\n",
        "def transform(encoding, data, vector_size=20):\n",
        "    transformed_data = np.zeros(shape=(len(data), vector_size))\n",
        "    for i in range(len(data)):\n",
        "        for j in range(min(len(data[i]), vector_size)):\n",
        "            try:\n",
        "                transformed_data[i][j] = encoding[data[i][j]]\n",
        "            except:\n",
        "                transformed_data[i][j] = encoding['<UNK>']\n",
        "    return transformed_data"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D24jWv0da8ts",
        "outputId": "c49ef5b7-8429-480c-d064-019c331faa10"
      },
      "source": [
        "#encoding training set\n",
        "encoded_training_input = transform(\n",
        "    encoding, training_input, vector_size=INPUT_LENGTH)\n",
        "encoded_training_output = transform(\n",
        "    encoding, training_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_training_input', encoded_training_input.shape)\n",
        "print('encoded_training_output', encoded_training_output.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_training_input (16000, 20)\n",
            "encoded_training_output (16000, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKr7NUk0bA5x",
        "outputId": "eea9d652-5386-4edc-abc2-c2287e4505b8"
      },
      "source": [
        "#encoding validation set\n",
        "encoded_validation_input = transform(\n",
        "    encoding, validation_input, vector_size=INPUT_LENGTH)\n",
        "encoded_validation_output = transform(\n",
        "    encoding, validation_output, vector_size=OUTPUT_LENGTH)\n",
        "\n",
        "print('encoded_validation_input', encoded_validation_input.shape)\n",
        "print('encoded_validation_output', encoded_validation_output.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_validation_input (4000, 20)\n",
            "encoded_validation_output (4000, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFwBzFKvEBLo"
      },
      "source": [
        "# Построение модели\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAa9-sybdrdZ"
      },
      "source": [
        "# Sequence-to-Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpJauy2PDywm"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "em56NODmVfTT"
      },
      "source": [
        "INPUT_LENGTH = 20\n",
        "OUTPUT_LENGTH = 20\n",
        "\n",
        "encoder_input = Input(shape=(INPUT_LENGTH,))\n",
        "decoder_input = Input(shape=(OUTPUT_LENGTH,))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pecsx1-nl8tV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1104a15-a4c4-4486-e1e7-f2a465acefdf"
      },
      "source": [
        "from keras.layers import SimpleRNN\n",
        "\n",
        "encoder = Embedding(dict_size, 128, input_length=INPUT_LENGTH, mask_zero=True)(encoder_input)\n",
        "encoder = LSTM(512, return_sequences=True, unroll=True)(encoder)\n",
        "encoder_last = encoder[:,-1,:]\n",
        "\n",
        "print('encoder', encoder)\n",
        "print('encoder_last', encoder_last)\n",
        "\n",
        "decoder = Embedding(dict_size, 128, input_length=OUTPUT_LENGTH, mask_zero=True)(decoder_input)\n",
        "decoder = LSTM(512, return_sequences=True, unroll=True)(decoder, initial_state=[encoder_last, encoder_last])\n",
        "\n",
        "print('decoder', decoder)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "encoder KerasTensor(type_spec=TensorSpec(shape=(None, 20, 512), dtype=tf.float32, name=None), name='lstm/transpose_2:0', description=\"created by layer 'lstm'\")\n",
            "encoder_last KerasTensor(type_spec=TensorSpec(shape=(None, 512), dtype=tf.float32, name=None), name='tf.__operators__.getitem/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem'\")\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "decoder KerasTensor(type_spec=TensorSpec(shape=(None, 20, 512), dtype=tf.float32, name=None), name='lstm_1/transpose_2:0', description=\"created by layer 'lstm_1'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lvHLUxUdQ2s"
      },
      "source": [
        "# Attention "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIr7K5j4mK2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd6f6ff-6206-43f3-a674-a7c97074e547"
      },
      "source": [
        "from keras.layers import Activation, dot, concatenate\n",
        "\n",
        "attention = dot([decoder, encoder], axes=[2, 2])\n",
        "attention = Activation('softmax', name='attention')(attention)\n",
        "print('attention', attention)\n",
        "\n",
        "context = dot([attention, encoder], axes=[2,1])\n",
        "print('context', context)\n",
        "\n",
        "decoder_combined_context = concatenate([context, decoder])\n",
        "print('decoder_combined_context', decoder_combined_context)\n",
        "\n",
        "output = TimeDistributed(Dense(512, activation=\"tanh\"))(decoder_combined_context)\n",
        "output = TimeDistributed(Dense(dict_size, activation=\"softmax\"))(output)\n",
        "print('output', output)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention KerasTensor(type_spec=TensorSpec(shape=(None, 20, 20), dtype=tf.float32, name=None), name='attention/Softmax:0', description=\"created by layer 'attention'\")\n",
            "context KerasTensor(type_spec=TensorSpec(shape=(None, 20, 512), dtype=tf.float32, name=None), name='dot_1/MatMul:0', description=\"created by layer 'dot_1'\")\n",
            "decoder_combined_context KerasTensor(type_spec=TensorSpec(shape=(None, 20, 1024), dtype=tf.float32, name=None), name='concatenate/concat:0', description=\"created by layer 'concatenate'\")\n",
            "output KerasTensor(type_spec=TensorSpec(shape=(None, 20, 1450), dtype=tf.float32, name=None), name='time_distributed_1/Reshape_1:0', description=\"created by layer 'time_distributed_1'\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDcTqdtmGhNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9be06b3-481e-4928-ddc6-0627f7a9711a"
      },
      "source": [
        "model = Model(inputs=[encoder_input, decoder_input], outputs=[output])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 20, 128)      185600      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     (None, 20, 512)      1312768     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 20, 128)      185600      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 512)          0           lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   (None, 20, 512)      1312768     embedding_1[0][0]                \n",
            "                                                                 tf.__operators__.getitem[0][0]   \n",
            "                                                                 tf.__operators__.getitem[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 20, 20)       0           lstm_1[0][0]                     \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "attention (Activation)          (None, 20, 20)       0           dot[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 20, 512)      0           attention[0][0]                  \n",
            "                                                                 lstm[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 20, 1024)     0           dot_1[0][0]                      \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 20, 512)      524800      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 20, 1450)     743850      time_distributed[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 4,265,386\n",
            "Trainable params: 4,265,386\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eIgCTDPWjcW"
      },
      "source": [
        "training_encoder_input = encoded_training_input\n",
        "training_decoder_input = np.zeros_like(encoded_training_output)\n",
        "training_decoder_input[:, 1:] = encoded_training_output[:,:-1]\n",
        "training_decoder_input[:, 0] = WORD_CODE_START\n",
        "training_decoder_output = np.eye(dict_size)[encoded_training_output.astype('int')]\n",
        "\n",
        "validation_encoder_input = encoded_validation_input\n",
        "validation_decoder_input = np.zeros_like(encoded_validation_output)\n",
        "validation_decoder_input[:, 1:] = encoded_validation_output[:,:-1]\n",
        "validation_decoder_input[:, 0] = WORD_CODE_START\n",
        "validation_decoder_output = np.eye(dict_size)[encoded_validation_output.astype('int')]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTCaMLtheYjz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ba1f3c-5c7e-4a53-9338-6d09021a1daa"
      },
      "source": [
        "model.fit(x=[training_encoder_input, training_decoder_input], y=[training_decoder_output],\n",
        "          validation_data=([validation_encoder_input, validation_decoder_input], [validation_decoder_output]),\n",
        "          #validation_split=0.05,\n",
        "          batch_size=64, epochs=100)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 32s 89ms/step - loss: 0.0023 - val_loss: 0.0020\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0020 - val_loss: 0.0019\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0017 - val_loss: 0.0016\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0014 - val_loss: 0.0017\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 0.0012 - val_loss: 0.0017\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0012 - val_loss: 0.0018\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0011 - val_loss: 0.0018\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.0010 - val_loss: 0.0019\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 9.9465e-04 - val_loss: 0.0019\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 9.5485e-04 - val_loss: 0.0019\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 9.1483e-04 - val_loss: 0.0020\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 8.7585e-04 - val_loss: 0.0020\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 8.3953e-04 - val_loss: 0.0020\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 8.0353e-04 - val_loss: 0.0021\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 7.6899e-04 - val_loss: 0.0021\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 7.3757e-04 - val_loss: 0.0022\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 7.0709e-04 - val_loss: 0.0022\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 6.7952e-04 - val_loss: 0.0022\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 6.4934e-04 - val_loss: 0.0023\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 6.2518e-04 - val_loss: 0.0023\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 6.0148e-04 - val_loss: 0.0023\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 5.7868e-04 - val_loss: 0.0023\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 5.6114e-04 - val_loss: 0.0024\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 5.4231e-04 - val_loss: 0.0024\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 5.2243e-04 - val_loss: 0.0024\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 5.0459e-04 - val_loss: 0.0025\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 4.9087e-04 - val_loss: 0.0025\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 4.8070e-04 - val_loss: 0.0025\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 4.7228e-04 - val_loss: 0.0026\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 4.6149e-04 - val_loss: 0.0026\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 4.4536e-04 - val_loss: 0.0026\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 4.2940e-04 - val_loss: 0.0026\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 4.1374e-04 - val_loss: 0.0027\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 3.9848e-04 - val_loss: 0.0027\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 3.8430e-04 - val_loss: 0.0027\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 3.7228e-04 - val_loss: 0.0027\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 3.7218e-04 - val_loss: 0.0028\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 3.7499e-04 - val_loss: 0.0028\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 3.8123e-04 - val_loss: 0.0028\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 3.6146e-04 - val_loss: 0.0028\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 3.3422e-04 - val_loss: 0.0029\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 3.0711e-04 - val_loss: 0.0029\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 2.8920e-04 - val_loss: 0.0029\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 2.7222e-04 - val_loss: 0.0029\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 2.6102e-04 - val_loss: 0.0030\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 2.6069e-04 - val_loss: 0.0030\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 2.8520e-04 - val_loss: 0.0030\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 3.4035e-04 - val_loss: 0.0030\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 3.2468e-04 - val_loss: 0.0030\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 2.6513e-04 - val_loss: 0.0030\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 2.1916e-04 - val_loss: 0.0031\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 1.9142e-04 - val_loss: 0.0031\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 1.7039e-04 - val_loss: 0.0031\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 1.5494e-04 - val_loss: 0.0032\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 1.4202e-04 - val_loss: 0.0032\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 1.3199e-04 - val_loss: 0.0032\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 1.2490e-04 - val_loss: 0.0033\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 1.8454e-04 - val_loss: 0.0033\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 3.8198e-04 - val_loss: 0.0032\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 2.6200e-04 - val_loss: 0.0032\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 1.6651e-04 - val_loss: 0.0032\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 1.1539e-04 - val_loss: 0.0033\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 8.8565e-05 - val_loss: 0.0033\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 7.0378e-05 - val_loss: 0.0033\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 5.8489e-05 - val_loss: 0.0034\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 21s 83ms/step - loss: 5.0194e-05 - val_loss: 0.0034\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 4.5105e-05 - val_loss: 0.0034\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 4.1141e-05 - val_loss: 0.0035\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 3.6734e-05 - val_loss: 0.0035\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 3.4392e-05 - val_loss: 0.0035\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 3.1966e-05 - val_loss: 0.0035\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 3.1550e-05 - val_loss: 0.0035\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 1.0589e-04 - val_loss: 0.0035\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 5.8068e-04 - val_loss: 0.0033\n",
            "Epoch 99/100\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 2.8128e-04 - val_loss: 0.0033\n",
            "Epoch 100/100\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 1.3897e-04 - val_loss: 0.0033\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1ef0343f10>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OztZQFnXegsN"
      },
      "source": [
        "def prediction(raw_input):\n",
        "    clean_input = clean_text(raw_input)\n",
        "    input_tok = [nltk.word_tokenize(clean_input)]\n",
        "    input_tok = [input_tok[0][::-1]]  #reverseing input seq\n",
        "    encoder_input = transform(encoding, input_tok, 20)\n",
        "    decoder_input = np.zeros(shape=(len(encoder_input), OUTPUT_LENGTH))\n",
        "    decoder_input[:,0] = WORD_CODE_START\n",
        "    for i in range(1, OUTPUT_LENGTH):\n",
        "        output = model.predict([encoder_input, decoder_input]).argmax(axis=2)\n",
        "        decoder_input[:,i] = output[:,i]\n",
        "    return output\n",
        "\n",
        "def decode(decoding, vector):\n",
        "    text = ''\n",
        "    for i in vector:\n",
        "        if i == 0:\n",
        "            break\n",
        "        text += ' '\n",
        "        text += decoding[i]\n",
        "    return text"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHHCsMUcejGz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cde30e2e-c1e9-4295-bc56-7860e89eee99"
      },
      "source": [
        "for i in range(20):\n",
        "    seq_index = np.random.randint(1, len(short_questions))\n",
        "    output = prediction(short_questions[seq_index])\n",
        "    print ('Questions:', short_questions[seq_index])\n",
        "    print ('Answers:', decode(decoding, output[0]))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions: because i like to torture you.\n",
            "Answers:  oh , bianca ? can you get me my <UNK> like my lives ?\n",
            "Questions: that is correct, sir.\n",
            "Answers:  sorry , doolittle . i have <UNK> so much since i have been in here . so much . .\n",
            "Questions: what are you doing, keitel blacksmith?\n",
            "Answers:  get away , <UNK> .\n",
            "Questions: you have tranquilizers?\n",
            "Answers:  i told you ! i have got everything !\n",
            "Questions: go ahead, bird dog.\n",
            "Answers:  you are all clear .\n",
            "Questions: he is not?\n",
            "Answers:  alex , has david <UNK> in his <UNK> <UNK> ?\n",
            "Questions: i am the stenographer.\n",
            "Answers:  in other words , no .\n",
            "Questions: what money?\n",
            "Answers:  do not bullshit me .\n",
            "Questions: now. one thing we need to discuss is timing. timing is absolutely crucial. what are you doing? anthony!\n",
            "Answers:  nothing . go ahead .\n",
            "Questions: guess it was not the pizza delivery guy.\n",
            "Answers:  you are lucky . i almost pulled the <UNK> .\n",
            "Questions: if it would not be too much of a bother... a little bite to eat would be...\n",
            "Answers:  oh , no bother , i would be glad to meet you . i owe him something anyway . .\n",
            "Questions: i mean did he take you up to his room with him? did you go to a hotel?\n",
            "Answers:  no . i think i will .\n",
            "Questions: there is no reason for fear.\n",
            "Answers:  good <UNK> , <UNK> .\n",
            "Questions: i really want to talk to you. i have been thinking about what you said about moving in here...\n",
            "Answers:  come on , harry , your <UNK> fuck through <UNK> ' at present 's <UNK> ... just give me me\n",
            "Questions: yes sir.\n",
            "Answers:  just perfect .\n",
            "Questions: i neither enjoy nor dislike. i do what is necessary.\n",
            "Answers:  how can you ? i mean they are people .\n",
            "Questions: everyone knew you would have been outspoken against this deal.\n",
            "Answers:  that is a fine <UNK> .\n",
            "Questions: but you are my father\n",
            "Answers:  never mind then , i will get in touch with him myself .\n",
            "Questions: ...not if i can help it.\n",
            "Answers:  <UNK> ! you will break a lot of <UNK> .\n",
            "Questions: it is cold.\n",
            "Answers:  i do not care .\n"
          ]
        }
      ]
    }
  ]
}