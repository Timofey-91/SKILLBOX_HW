{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "mid_ml_nlp_les-3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZbioSC2r-bm"
      },
      "source": [
        "# Урок 3 Векторизация текста: Bag of Words\n",
        "\n",
        "Итак, мы умеем подготавливать текст к обработке: приводить слова к начальным формам, разделять текст на токены, удалять \"мусорные\" токены (стоп-слова). Однако, мы знаем, что нейросети работают не с текстом, а с числами. Давайте разбираться, как переводить токены в числа, то есть с тем, как работает векторизация."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhEXQdDLr-bp"
      },
      "source": [
        "Bag of Words - это способ перейти от набора токенов к численному вектору. Алгоритм векторизации текста по модели BoW:\n",
        "\n",
        "1. определяем количество $N$ различных токенов во всех доступных текста - так называемый \"словарь\".\n",
        "1. присваиваем каждому токену случайный номер от $0$ до $N$.\n",
        "1. для каждого документа $i$ формируем вектор размерности $N$ - ставим на позицию $j$ количество вхождений токена с номером $j$, которые содержатся в тексте $i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEtSB34fr-bq"
      },
      "source": [
        "Каждый токен мы по сути представляем в виде вектора размерности $N$, который состоит из нулей и всего одной единицы, такое кодирование называется *One-Hot encoding*. А каждый документ это \"сумма\" всех one-hot векторов входящих в него токенов.\n",
        "\n",
        "Такой подход хорошо иллюстрируется картинкой:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lmBZLBtr-br"
      },
      "source": [
        "![bow](https://sun9-2.userapi.com/c854228/v854228722/1f4c57/BWDIDvXh-ew.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SQXC6Vhr-bs"
      },
      "source": [
        "Для каждого элемента получаем вектор из нулей и единиц. При этом размерность словаря обычно составляет несколько десятков тысяч, а количество токенов в одном документе несколько десятков - то есть нулей значительно больше, чем единиц - такие данные называются *разреженными*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X18cqrKEr-bu"
      },
      "source": [
        "В таком виде данные уже пригодны для работы с нейросетью или любым другим алгоритмом ML, однако есть несколько довольно простых и полезных вещей, которые мы можем сделать и без нейросетей. Давайте сначала разберем их, а потом вернемся к нейросетям. Такое представление текста позволяет решать интересные задачи - например, находить самые похожие друг на друга тексты. Чтобы как-то формализовать понятие \"схожести\" текстов, вводится понятие *косинусного расстояния* между двумя векторами текстов $a$ и $b$ размерности $N$. С этой метрикой вы [можете познакомиться в Википедии](https://ru.wikipedia.org/wiki/Векторная_модель#Косинусное_сходство ), формула такая для двух векторов $a$ и $b$ с координатами $a_i$ и $b_i$ соответственно:\n",
        "$$\n",
        "\\text{similarity} = \\cos (\\theta) = 1 - \\frac{\\sum_{i=1}^{N}a_ib_i}{\\sqrt{\\sum_{i=1}^{N}(a_i)^2}\\sqrt{\\sum_{i=1}^{N}(b_i)^2}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLQA1DoTr-bv"
      },
      "source": [
        "Интуитивное объяснение для простого случая: два документа полностью совпадают, тогда единички в них стоят на одних и тех же местах - расстояние между ними будет нулевым. Если два текста совершенно не пересекаются, то единички будут стоять на разных местах - расстояние в этом случае равно единице. Самостоятельно реализовывать функцию не нужно - есть готовая реализация в [scipy.spatial.distance.cosine](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.cosine.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdnluxQyr-bw"
      },
      "source": [
        "Векторизуем наш корпус (набор текстов) с помощью класса `CountVectorizer()` (то есть превращаем наборы токенов в наборы векторов)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiD5XbFrr-bx",
        "outputId": "eac7d2d7-f1ab-4fc8-da12-5b39ecf0e465"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# инициализируем объект, который токенизирует наш текст\n",
        "# в качестве единственного аргимента передаём функцию, которую мы написали в Уроке 2\n",
        "# на разбивает каждый документ на токены\n",
        "vectorizer = CountVectorizer(tokenizer=tokenize_text)\n",
        "# применяем наш объект-токенизатор к датафрейму с твитами\n",
        "document_matrix = vectorizer.fit_transform(df.tweet_text.values)\n",
        "# результат - матрица, в которой находятся числа, строк в мастрице столько, сколько документов\n",
        "# а столбцов столько, сколько токенов\n",
        "document_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<3904x7309 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 46042 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rArPxKfEr-b6"
      },
      "source": [
        "Класс `sklearn.feature_extraction.text.CountVectorizer` реализует алгоритм преобразования массива текстовых документов в разреженную матрицу такую, что\n",
        "\n",
        "* число строк совпадает с количеством документов в исходном датафрейме\n",
        "* количество столбцов совпадает с количеством различных токенов\n",
        "* объект `CountVectorizer()` содержит в себе разные вспомогательные элементы - например, словарь соответствия токена и его номера\n",
        "\n",
        "Полученные вектора можно использовать в алгоритмах второго уровня - например, в задаче классификации отзывов.\n",
        "\n",
        "Пользуясь матрицей, найдем твит, который максимально похож на первый твит из набора:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2nG-JxQr-b7",
        "outputId": "33090048-394e-46bb-918c-4fea587e089b"
      },
      "source": [
        "source_tweet_index = 0\n",
        "print(df.iloc[source_tweet_index].tweet_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-WNLcX_r-b_"
      },
      "source": [
        "Вычисляем попарные схожести между элементами разреженной матрицы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSMqxf4Mr-cA",
        "outputId": "624d94ec-df7c-4710-a3aa-9a78ba1316b9"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "tweet_distance = 1-pairwise_distances(document_matrix, metric=\"cosine\")\n",
        "\n",
        "tweet_distance.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4499, 4499)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUrOzDRqr-cE"
      },
      "source": [
        "Мы получили квадратную матрицy, которая содержит столько строк и столбцов, сколько документов в нашем  корпусе  (наборе текстов)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJV9ebsfr-cF",
        "outputId": "cdb290aa-c25e-45fc-a142-21deb51bbcf5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# отсортируем твиты по “похожести” - чем похожее на source_tweet_index,\n",
        "# тем ближе к началу списка sorted_similarity\n",
        "sorted_similarity = np.argsort(-tweet_distance[source_tweet_index,:])\n",
        "\n",
        "sorted_similarity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0, 633, 420, ...,  47, 572, 793])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXEMxevVr-cJ"
      },
      "source": [
        "Мы получили вектор \"схожестей\", который содержит индексы похожих твитов, расположенных по убыванию схожести. Больше всего твит похож сам на себя, поэтому возьмём индекс второго по схожести элемента (и далее)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHcA83flr-cK",
        "outputId": "20345c55-5af5-4780-b044-f6e711eb29b0"
      },
      "source": [
        "print(df.iloc[0].tweet_text)\n",
        "print('-------------')\n",
        "print(df.iloc[sorted_similarity[1]].tweet_text)\n",
        "print('-------------')\n",
        "print(df.iloc[sorted_similarity[2]].tweet_text)\n",
        "print('-------------')\n",
        "print(df.iloc[sorted_similarity[3]].tweet_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.\n",
            "-------------\n",
            ".@mention I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.\n",
            "-------------\n",
            "IPhone is dead. Find me on the secret batphone #sxsw.\n",
            "-------------\n",
            "The big takeaway from #SXSW interactive - I need an iphone.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSKqP6olr-cN"
      },
      "source": [
        "Мы получили мощный инструмент для анализа текстов - например, мы случайно нашли дубликат твита"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d29Mdz4cr-cO"
      },
      "source": [
        "Кроме простого подхода, когда мы вычисляем счётчик вхождения токена, можно вычислять более сложную метрику TF-IDF (term frequency - inverse document frequency), которая вычисляется по следующей формуле для токена $t$ и документа $d$:\n",
        "$$\n",
        "\\text{tf-idf}(t,d) = \\text{tf}(t,d)\\cdot\\text{idf}(t)\n",
        "$$\n",
        "\n",
        "Где $\\text{tf}(t,d)$ - элемент матрицы, полученной из `CountVectorizer()`, который мы умножаем на величину $\\text{idf}(t)$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx7N1gVgr-cP"
      },
      "source": [
        "Эта величина показывает количество документов в корпусе  (наборе текстов), в которых был встречен токен $t$:\n",
        "$$\n",
        "\\text{idf}(t) = \\log\\frac{1+N}{1+\\text{df(t)}} + 1\n",
        "$$\n",
        "\n",
        "где $\\text{df}(t)$ - количество документов корпуса, в которых был встречен токен $t$. Таким образом мы понижаем веса у слов, которые встречаются почти во всех документах - такие токены являются неинформативными и мусорными, алгоритм понижает их \"важность\" для анализа."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuH0kEkNr-cR"
      },
      "source": [
        "Алгоритм TF-IDF лучше подходит для анализа текстов и даёт более высокое качество, но более затратен по вычислениям. Как выбрать между этими алгоритмами?\n",
        "\n",
        "* если токенов менее 10000 используйте TF-IDF.\n",
        "* если токенов более 10000 то *попробуйте* использовать TF-IDF, если не получится - возвращайтесь к CountVectorizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3CphJB_r-cS"
      },
      "source": [
        "**Недостатки BoW подхода** Используя алгоритмы вроде Вag of Words, мы теряем порядок слов в тексте, а значит, тексты \"i have no cows\" и \"no, i have cows\" будут идентичными после векторизации, хотя и противоположными семантически. Чтобы избежать этой проблемы, можно сделать шаг назад и изменить подход к токенизации: например, использовать N-граммы (комбинации из N последовательных токенов). Обычно по корпусу  (набору текстов) формируются биграммы (последовательности из двух слов) или триграммы (последовательности из трёх слов).\n",
        "\n",
        "Кроме того, текст можно разбивать не на слова, а на последовательности букв - при таком подходе опечатки будут автоматически учитываться."
      ]
    }
  ]
}