{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "4_Exploration.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgshItiNbYmH"
      },
      "source": [
        "# Реализация Exploration в Q-Learning\n",
        "\n",
        "В этом уроке мы рассмотрим пример реализации Epsilon-greedy подхода для добавления Exploration составляющей в Q-Learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1c9g1MHeUCl"
      },
      "source": [
        "### Epsilon-greedy подход\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrYHIURZ6_2Y"
      },
      "source": [
        "Вот так выглядит eps-greedy стратегия выбора действия при обучении Q-функции. Пусть есть некоторый фиксированный параметр `eps`, тогда с вероятностью `eps` мы выбираем случайное действие, а с веротяностью `1-eps` выбираем действие исходя из текущей полтики (как в обычном Q-Learning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-zMowjHFz3t"
      },
      "source": [
        "if np.random.rand() < eps:\n",
        "    # Выбор случайного действия\n",
        "    # ...\n",
        "else:\n",
        "    # Выбор действия по текущей политике\n",
        "    # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a7hF7Ha7A18"
      },
      "source": [
        "Как выбирать параметр `eps`? Обычно его меняют во времени в процессе обучения. \n",
        "\n",
        "Вначале, пока Q-функция еще не особо обучена, имеет смысл больше исследовать и меньше полагаться на Q-функцию. То есть сделать `eps` большим. Агент будет совершать больше случайных действий, часто ошибаться, сворачивать не туда, падать в яму, зато будет исследовать среду, искать новые траектории, получать опыт.\n",
        "\n",
        "А ближе к концу обучения нужно меньше исследовать (меньше случайных действий) и больше полагаться на оптимальную политику (полученную из уже хорошо обученной Q-функции). Поэтому ближе к концу обучения параметр `eps` надо снижать (до нуля).\n",
        "\n",
        "Таким образом, можно сделать так, чтобы параметр `eps` линейно убывал с номером эпизода."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsKuiRit6Vss"
      },
      "source": [
        "# цикл Q-Learning по эпизодам\n",
        "for i in range(NUM_EPISODES):\n",
        "    eps = 1.0 - float(i) / NUM_EPISODES\n",
        "    # ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAGOjhbmg409"
      },
      "source": [
        "**[Задание 1]** Обучите агента для игры `FrozenLake-v0` с помощью табличного Q-Learning и с помощью eps-greedy подхода, описанного выше. Подберите гиперпараметры таким образом, чтобы обученный агент в итоге доходил до цели."
      ]
    }
  ]
}