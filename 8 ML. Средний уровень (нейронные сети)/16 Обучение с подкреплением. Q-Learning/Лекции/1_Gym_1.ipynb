{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "1_Gym_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgshItiNbYmH"
      },
      "source": [
        "# Знакомство с Gym\n",
        "\n",
        "В этом уроке мы познакомимся с удобной средой для проведения RL экспериментов под названием Gym (https://gym.openai.com/). По аналогии с тем, как для обучения с учителем нам надо где-то брать обучающие датасеты, для обучения с подкреплением нам нужно иметь среду с определёнными правилами (или симуляцию это среды), в которой действует агент. \n",
        "\n",
        "в Gym содержатся реализации различных игр и других симуляций, на которых можно проводить эксперименты в области RL. Кроме самих симуляций в Gym имеется удобная обёртка для доступа к симуляции: мы сразу можем оперировтаь в терминах RL (состояние, действие, итд)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1c9g1MHeUCl"
      },
      "source": [
        "### Загрузка библиотек\n",
        "\n",
        "Загружаем библиотеку gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-zMowjHFz3t"
      },
      "source": [
        "import gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAGOjhbmg409"
      },
      "source": [
        "### Создание игровой среды\n",
        "\n",
        "С помощью функции `gym.make` мы можем создать симулятор необходимой нам игры. Рассмотрим пример игры `Frozen Lake`. Здесь есть ледяное поле и ямы (holes). Цель игры дойти до целевой позиции, не упав в яму. У этой игры есть парамтер `is_slippery`, который означает, \"будет ли лёд скользким\" -- будет ли среда всегда со стопроцентной вероятностью подчиняться нашему действию. Для простоты отключим этот флаг (куда захотели пойти, там и оказались).\n",
        "\n",
        "Другие доступные симуляции: https://gym.openai.com/envs/\n",
        "\n",
        "Посмотрим, сколько есть возможных действий и состояний в нашей игре. Кол-во состояний 16, так как столько ячеек в поле (потенциальных позиций для агента). А действий 4 (4 направления)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIprlbAyJo18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d3f8c2f1-a0d8-469a-9386-13c77b205315"
      },
      "source": [
        "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
        "\n",
        "NUM_STATES = env.observation_space.n\n",
        "NUM_ACTIONS = env.action_space.n\n",
        "\n",
        "print('States: {}'.format(NUM_STATES))\n",
        "print('Actions: {}'.format(NUM_ACTIONS))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "States: 16\n",
            "Actions: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJxgn6Uowdex"
      },
      "source": [
        "### Основные функции Gym"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI-7omyUwUqE"
      },
      "source": [
        "С помощью `env.reset()` можно перезапустить среду в исходное состояние (на начало эпизода). Эта функция также вернет начальное состояние.\n",
        "\n",
        "В нашем случае состояние это просто число -- индекс соответствующей ячейки, где находится робот."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcUWhhTwHLyy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e7f1fb0-cb1b-4a11-d43d-8d6474119c6f"
      },
      "source": [
        " s = env.reset()\n",
        " print(s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tfMM2sVw1fG"
      },
      "source": [
        "С помощью функции `env.render()` можно визуализировать текущее состояние среды. В Colab это не всегда можно сделать довольно просто (в случае сложных симуляций), но в случае Frozen Lake это просто напечатанный текст с нашим полем 4x4. S - start, F - frozen, H - hole, G - goal. Маркером указано положение робота."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LvlYZK7NMVo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "16a95266-f9fe-406e-edb0-836f046c903b"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnFEuExKxmie"
      },
      "source": [
        "Действия тоже кодируруются соответствующим индексом.\n",
        "Можно, например, выбрать случайное действие с попомщью функции `env.action_space.sample()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxQjACvNHNul",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "27cc2461-d23c-4e63-b520-d524fb2433aa"
      },
      "source": [
        "a = env.action_space.sample()\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vtt-jbKsxoWy"
      },
      "source": [
        "Чтобы совершить действие `a` нужно вызвать функцию `env.step(a)`. Эта функция вернет новое состояние (`s1`), в которое мы перешли, награду `r`, информацию о том, завершилась ли игра (`done`) и другую менее важную информацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD-3K5-xHP8y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c49345e5-d3cc-42a9-dc10-42c29f20ea42"
      },
      "source": [
        "s1, r, done, _ = env.step(a)\n",
        "\n",
        "print('New state: ', s1)\n",
        "print('Reward: ', r)\n",
        "print('Done? ', done)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New state:  1\n",
            "Reward:  0.0\n",
            "Done?  False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHjB31w2yVxY"
      },
      "source": [
        "Посмотрим, как теперь выглядит текущее состояние."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hn3ubSqNhDe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d709861e-608d-4b18-d93a-9f3df609d442"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEJkMNdoycst"
      },
      "source": [
        "### Запуск симуляции\n",
        "\n",
        "Теперь у нас есть все знания, чтобы сыграть целый игровой эпизод с помощью Gym. Сначала встаём в стартовую позицию `env.reset()`. Потом в цикле совершаем шаги и рисуем промежуточные состояние с помощью `env.render()`. На каждом шаге нам как-то надо выбрать действие. Выбор действия обернём в функцию `a = policy(s)`. В идеале мы должны руководстсоваться некой стратегией, но в этом простом примере будем выбирать случайные действия `a`. После выбора действия `a` делаем шаг `env.step(a)` (сообщаем среде наше желание сделать действие). Если после определённого шага среда вернула `done=True`, значит произошел конец эпизода (упали в яму или дошли до цели). Если мы дошли до цели, то на последнем шаге мы должны были получить ненулевую награду."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r82qS-BZHU5i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 953
        },
        "outputId": "094cef8f-d0b8-4450-f31b-98971fcd4793"
      },
      "source": [
        "def policy(s):\n",
        "    a = env.action_space.sample() # случайная стратегия\n",
        "    return a\n",
        "\n",
        "s = env.reset()\n",
        "\n",
        "for _ in range(100):\n",
        "    env.render()\n",
        "    a = policy(s)\n",
        "    s, r, done, _ = env.step(a)\n",
        "    print('Reward = {}'.format(r))\n",
        "    if done:\n",
        "        env.render()\n",
        "        print('Final reward = {}'.format(r))\n",
        "        break\n",
        "        \n",
        "env.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Up)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "Reward = 0.0\n",
            "  (Down)\n",
            "SFFF\n",
            "F\u001b[41mH\u001b[0mFH\n",
            "FFFH\n",
            "HFFG\n",
            "Final reward = 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q1RuTqqzk9A"
      },
      "source": [
        "**[Задание 1]** Запрограммируйте бота вручную на совершение таких действий, при которых он дойдёт до цели и получит ненулевую награду. Для этого измените лишь функцию `policy()`. Просто создайте набор правил (стратегию) -- какое действие надо совершить в зависимости от состояния. Проведите симуляцию с использованием вашей стратегии и посмотрите на результат."
      ]
    }
  ]
}